{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from  datetime import datetime, timedelta\nimport gc\nimport numpy as np, pandas as pd\nimport lightgbm as lgb","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import datetime\n# h = 28 \n# max_lags = 57\n# tr_last = 1941 #1913\n# fday = datetime.datetime(2016,5,24) #(2016,4, 25) \n# fday","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #create dt\n# sales = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\", usecols = catcols + numcols, dtype = dtype)\n# d[f'd_{day} for day in range(1, 1914)]\n# for d in range(1942, 1970):\n#     col = 'd_' +str(d)\n#     sales[col] = 0\n# sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create dt\nnumcols = [f\"d_{day}\" for day in range(1000,1942)]\ncatcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\ndtype = {numcol:\"int16\" for numcol in numcols} \ndtype.update({col: \"category\" for col in catcols if col != \"id\"})\n\n\nsales = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\", usecols = catcols + numcols, dtype = dtype)\n\nfor d in range(1942, 1970):\n    col = 'd_' +str(d)\n    sales[col] = 0\nsales.head()\n  \nfor col in catcols:\n    if col != \"id\":\n        sales[col] = sales[col].cat.codes.astype(\"int16\")\n        sales[col] -= sales[col].min()\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# m = []\n# for x in range(495,1970):\n#     x = str(x)\n#     m.append(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #m = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n# sales.columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+ m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def melt_and_merge(dt):\n    prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n            \n    cal = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n            \n            \n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dt = pd.melt(dt,\n                id_vars = catcols,\n                value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                var_name = \"d\",\n                value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    return dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_fea(dt):\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n            #df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)\n    \n    dt['expanding_sold_mean'] = dt.groupby(['id'])['sales'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)#accumulative 每次加下面一个\n    \n    dt['state_cat_sold_avg'] = dt.groupby(['state_id','cat_id'])['sales'].transform('mean').astype(np.float16)\n    dt['state_dept_sold_avg'] = dt.groupby(['state_id','dept_id'])['sales'].transform('mean').astype(np.float16)\n    \n    dt['store_cat_sold_avg'] = dt.groupby(['store_id','cat_id'])['sales'].transform('mean').astype(np.float16)\n    dt['store_dept_sold_avg'] = dt.groupby(['store_id','dept_id'])['sales'].transform('mean').astype(np.float16)\n    \n    \n    dt['daily_avg_sales'] = dt.groupby(['id','d'])['sales'].transform('mean').astype(np.float16)\n    dt['avg_sales'] = dt.groupby(['id'])['sales'].transform('mean').astype(np.float16)\n    dt['selling_trend'] = (dt['daily_avg_sales'] - dt['avg_sales']).astype(np.float16)\n    dt.drop(['daily_avg_sales','avg_sales'],axis=1,inplace=True)\n    \n    \n    \n    \n    \n#     date_features = {\n        \n#         \"wday\": \"weekday\",\n#         \"week\": \"weekofyear\",\n#         \"month\": \"month\",\n#         \"quarter\": \"quarter\",\n#         \"year\": \"year\",\n#         \"mday\": \"day\",\n# #         \"ime\": \"is_month_end\",\n# #         \"ims\": \"is_month_start\",\n#     }\n    \n    dt.drop([\"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n#     for date_feat_name, date_feat_func in date_features.items():\n#         if date_feat_name in dt.columns:\n#             dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n#         else:\n#             dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n    return dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = melt_and_merge(sales)\ndt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sales; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = create_fea(dt)\ndt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.d = dt.d.apply(lambda x: x.split('_')[-1]).astype('int16')\ndt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dt.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train = dt[dt['d']<1914].drop('sales',axis=1), dt[dt['d']<1914]['sales']\nX_valid, y_valid = dt[(dt['d']>=1914) & (dt['d']<1942)].drop('sales',axis=1), dt[(dt['d']>=1914) & (dt['d']<1942)]['sales']\nX_test = dt[dt['d']>=1942].drop('sales',axis=1)\nX_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.drop(['date','id'], axis=1,inplace=True)\ndt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\"objective\" : \"poisson\",\n          \"metric\" :\"rmse\",\n          #\"force_row_wise\" : True, #enabling this is recommended when:\n# the number of columns is large, or the total number of bins is large\n# num_threads is large, e.g. >20\n# you want to reduce memory cost\n          \"learning_rate\" : 0.075, #learning_rate > 0.0Typical: 0.05.\n#\"sub_feature\" : 0.8,#if you set it to 0.8, LightGBM will select 80% of features; alias 'feature_fraction' default as 1\n          \"sub_row\" : 0.75, #0.0 < bagging_fraction <= 1.0\n# like feature_fraction, but this will randomly select part of data without resampling\n# can be used to speed up training\n# can be used to deal with over-fitting\n# Note: to enable bagging, bagging_freq should be set to a non zero value as well\n#\"bagging_fraction\": 1 #to enable bagging, bagging_fraction should be set to value smaller than 1.0 as well\n          \"bagging_freq\" : 1, #0 means disable bagging; k means perform bagging at every k iteration\n    #to enable bagging, bagging_fraction should be set to value smaller than 1.0 as well. defaul as 0\n          \"lambda_l2\" : 0.1, #alias reg_alpha, >=1, regularization\n          \"nthread\" : 4, #real CPU cores\n            #\"metric\": [\"rmse\"],\n            #'verbosity': 1,\n          'num_iterations' : 1200, #use smaller learning_rate with larger num_iterations. Also, you should use early_stopping_rounds if you go for higher num_iterations to stop your training when it is not learning anything useful.\n          'early_stopping_rounds': 80, #The rule of thumb is to have it at 10% of your num_iterations.\n    'num_leaves': 128, #1 < num_leaves <= 131072 max # of leaves in one tree, controling depth, overfitting.A higher value results in deeper trees.\n    #num_leaves = 2^(max_depth); It is necessary to tune num_leaves with the max_depth together.\n    \"min_data_in_leaf\": 100, #default as 20, >0, control overfitting. min_data_in_leaf depends on the number of training samples and num_leaves\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\ntrain_data = lgb.Dataset(X_train , label = y_train, \n                         categorical_feature=cat_feats, free_raw_data=False)#which one is categorical feature\nvalid_data = lgb.Dataset(X_valid, label = y_valid,\n                              categorical_feature=cat_feats,\n                 free_raw_data=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nm_lgb = lgb.train(params, train_data, valid_sets = valid_data, verbose_eval=200) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_lgb.save_model(\"model.lgb\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = m_lgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = [round(a, 4) for a in y_test]\ny_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = np.reshape(y_test, (30490,28))\nm = pd.DataFrame(result)\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\nm\n#f'F{i}' for i in range(1,29)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m['id']=sub.id[30491:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation = sales[['id']+['d_' + str(i) for i in range(1914,1942)]]\n\nvalidation['id']=sub.id[:30491]\nvalidation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\nvalidation","execution_count":8,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n","name":"stderr"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"                                  id  F1  F2  F3  F4  F5  F6  F7  F8  F9  ...  \\\n0      HOBBIES_1_001_CA_1_validation   0   0   0   2   0   3   5   0   0  ...   \n1      HOBBIES_1_002_CA_1_validation   0   1   0   0   0   0   0   0   0  ...   \n2      HOBBIES_1_003_CA_1_validation   0   0   1   1   0   2   1   0   0  ...   \n3      HOBBIES_1_004_CA_1_validation   0   0   1   2   4   1   6   4   0  ...   \n4      HOBBIES_1_005_CA_1_validation   1   0   2   3   1   0   3   2   3  ...   \n...                              ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...   \n30485    FOODS_3_823_WI_3_validation   0   0   0   2   2   0   0   0   2  ...   \n30486    FOODS_3_824_WI_3_validation   0   1   1   1   0   0   0   0   1  ...   \n30487    FOODS_3_825_WI_3_validation   0   0   1   1   0   2   1   1   0  ...   \n30488    FOODS_3_826_WI_3_validation   1   3   0   1   2   1   0   2   1  ...   \n30489    FOODS_3_827_WI_3_validation   0   0   0   0   0   1   1   1   2  ...   \n\n       F19  F20  F21  F22  F23  F24  F25  F26  F27  F28  \n0        2    4    0    0    0    0    3    3    0    1  \n1        0    1    2    1    1    0    0    0    0    0  \n2        1    0    2    0    0    0    2    3    0    1  \n3        1    1    0    4    0    1    3    0    2    6  \n4        0    0    0    2    1    0    0    2    1    0  \n...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n30485    1    0    3    0    1    1    0    0    1    1  \n30486    0    0    0    0    0    0    1    0    1    0  \n30487    0    0    1    2    0    1    0    1    0    2  \n30488    1    1    1    4    6    0    1    1    1    0  \n30489    1    2    0    5    4    0    2    2    5    1  \n\n[30490 rows x 29 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>F1</th>\n      <th>F2</th>\n      <th>F3</th>\n      <th>F4</th>\n      <th>F5</th>\n      <th>F6</th>\n      <th>F7</th>\n      <th>F8</th>\n      <th>F9</th>\n      <th>...</th>\n      <th>F19</th>\n      <th>F20</th>\n      <th>F21</th>\n      <th>F22</th>\n      <th>F23</th>\n      <th>F24</th>\n      <th>F25</th>\n      <th>F26</th>\n      <th>F27</th>\n      <th>F28</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HOBBIES_1_001_CA_1_validation</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HOBBIES_1_002_CA_1_validation</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HOBBIES_1_003_CA_1_validation</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HOBBIES_1_004_CA_1_validation</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>4</td>\n      <td>1</td>\n      <td>6</td>\n      <td>4</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HOBBIES_1_005_CA_1_validation</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30485</th>\n      <td>FOODS_3_823_WI_3_validation</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30486</th>\n      <td>FOODS_3_824_WI_3_validation</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30487</th>\n      <td>FOODS_3_825_WI_3_validation</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>30488</th>\n      <td>FOODS_3_826_WI_3_validation</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30489</th>\n      <td>FOODS_3_827_WI_3_validation</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>30490 rows × 29 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.concat([validation, m], axis=0, sort=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}